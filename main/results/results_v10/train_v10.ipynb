{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr_GsG0m2Yn7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import shutil\n",
        "import yaml\n",
        "import ast\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDMJXF4BbLaW",
        "outputId": "f09449b1-7efd-401e-c4c0-80931ed28281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-b2JQzE1tP0"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "IMGSZ = 320\n",
        "MODEL = \"v10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Il3qvvGV66B",
        "outputId": "6da8c82d-c99a-443d-a0f8-ab32e4551906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.1.7 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=/content/data/yolocustom.yaml, epochs=10, time=None, patience=5, batch=256, imgsz=320, save=True, save_period=-1, cache=True, device=0, workers=16, project=v10, name=train, exist_ok=True, pretrained=False, optimizer=Adam, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=None, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=v10/train\n",
            "Overriding model.yaml nc=80 with nc=392\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   4002664  ultralytics.nn.modules.head.Detect           [392, [192, 384, 576]]        \n",
            "YOLOv8m summary: 295 layers, 26083288 parameters, 26083272 gradients, 80.3 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir v10/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/yolo/train.cache... 2015880 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2015880/2015880 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/010502_4019990013590.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/020101_4020200020056.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/020301_4020200020056.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/020915_1409517.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/030418_4119950011957.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/030418_4119960004568.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/030418_4119980005417.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/230101_1061409.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/230101_4020040029622.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/250118_4020080045624.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/250118_4020100024748.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/250125_4020130066871.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/250125_4020130066872.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/250125_4020130066873.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260106_4020160067280.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260106_4119980002797.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260115_1127728A.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260115_1228427.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260118_4019960056782.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260318_4019860013217.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1003483.jpg: 10 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1063120.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1066181.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1091795.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1110979.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1170243.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1213974.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1221287.jpg: 5 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1268374.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1283587.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1288753.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1357475.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1358986.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1381272.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1397761.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1431364.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1459096.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1466788.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_1488239.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260405_4020200051470.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260406_1356616.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260409_1260954.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260409_1476975.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260412_4020080037546.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260418_4019990030063.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260418_4020140010234.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260418_4020190122363.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260418_4020190188620.jpg: 3 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/260719_4020200021409.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/261109_1418664.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/261525_1241670.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/270505_4019860012290.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/270907_4020080031450.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/yolo/train/270909_4020150063336.jpg: 1 duplicate labels removed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (576.7GB True): 100%|██████████| 2015880/2015880 [17:21<00:00, 1935.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/yolo/valid.cache... 33816 images, 0 backgrounds, 0 corrupt: 100%|██████████| 33816/33816 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (9.7GB True): 100%|██████████| 33816/33816 [00:34<00:00, 978.43it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to v10/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.002), 83 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 320 train, 320 val\n",
            "Using 12 dataloader workers\n",
            "Logging results to \u001b[1mv10/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/10      31.3G      1.383      3.982        1.7        157        320: 100%|██████████| 7875/7875 [1:19:07<00:00,  1.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:21<00:00,  2.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.404     0.0716     0.0439     0.0287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/10      31.7G       1.18      3.428      1.518        156        320: 100%|██████████| 7875/7875 [1:22:28<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:15<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030       0.36      0.118     0.0724     0.0478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/10      31.6G      1.159      3.339      1.492        172        320: 100%|██████████| 7875/7875 [1:22:09<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:07<00:00,  1.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.478     0.0723     0.0653     0.0424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/10      31.7G      1.142      3.274      1.472        161        320: 100%|██████████| 7875/7875 [1:22:17<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:01<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030     0.0236      0.429     0.0516     0.0327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/10      31.6G      1.135      3.247      1.462        144        320: 100%|██████████| 7875/7875 [1:21:06<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:05<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030     0.0241      0.467     0.0566     0.0365\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/10      31.5G      1.122      3.195       1.45        169        320: 100%|██████████| 7875/7875 [1:20:39<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:06<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030     0.0825      0.327     0.0689      0.045\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/10      31.7G      1.109      3.146      1.439        168        320: 100%|██████████| 7875/7875 [1:20:47<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:11<00:00,  1.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030       0.48     0.0701     0.0797     0.0524\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/10      31.6G      1.097      3.094      1.427        160        320: 100%|██████████| 7875/7875 [1:20:34<00:00,  1.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:02<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.414     0.0977     0.0892     0.0589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/10      31.7G      1.083      3.035      1.417        182        320: 100%|██████████| 7875/7875 [1:19:49<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:14<00:00,  2.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.361      0.119     0.0975     0.0645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/10      31.7G      1.065      2.963      1.405        150        320: 100%|██████████| 7875/7875 [1:21:57<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:08<00:00,  1.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.339      0.136      0.104     0.0695\n",
            "\n",
            "10 epochs completed in 13.924 hours.\n",
            "Optimizer stripped from v10/train/weights/last.pt, 52.4MB\n",
            "Optimizer stripped from v10/train/weights/best.pt, 52.4MB\n",
            "\n",
            "Validating v10/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.7 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "YOLOv8m summary (fused): 218 layers, 26066728 parameters, 0 gradients, 79.9 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 67/67 [02:41<00:00,  2.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      33816      39030      0.322      0.151      0.106     0.0716\n",
            "                010102      33816        100        0.1       0.61      0.147     0.0831\n",
            "                010103      33816        100          1          0    0.00894    0.00402\n",
            "                010104      33816        100      0.167       0.01     0.0201     0.0129\n",
            "                010105      33816        100     0.0978       0.12     0.0504     0.0321\n",
            "                010109      33816        100       0.13       0.24     0.0901     0.0548\n",
            "                010110      33816        100      0.535       0.09       0.14     0.0867\n",
            "                010112      33816        100      0.706     0.0248     0.0981     0.0567\n",
            "                010114      33816        100      0.628       0.18      0.254      0.167\n",
            "                010115      33816        100      0.146    0.00292      0.148     0.0762\n",
            "                010301      33816        100      0.161       0.06     0.0693     0.0387\n",
            "                010306      33816        100      0.278       0.28      0.152     0.0998\n",
            "                010315      33816        100      0.101       0.43      0.141      0.104\n",
            "                010316      33816        100       0.54       0.29      0.321      0.244\n",
            "                010317      33816        100      0.473       0.22      0.262        0.2\n",
            "                010502      33816        100      0.135       0.28     0.0995     0.0688\n",
            "                010706      33816        100      0.545       0.06      0.108     0.0855\n",
            "                011315      33816        100      0.317      0.375      0.266      0.171\n",
            "                011503      33816        100      0.171       0.03      0.053     0.0345\n",
            "                011505      33816        100      0.329      0.147      0.116     0.0681\n",
            "                011507      33816        100     0.0929       0.02     0.0424     0.0289\n",
            "                011509      33816        100          1          0    0.00908    0.00695\n",
            "                011511      33816        100      0.081       0.16     0.0355     0.0175\n",
            "                011515      33816        100     0.0551       0.14     0.0318     0.0212\n",
            "                011517      33816        100      0.163       0.08     0.0457     0.0287\n",
            "                011521      33816        100      0.215       0.33       0.22      0.167\n",
            "                011523      33816        100       0.13       0.34       0.14      0.111\n",
            "                011524      33816        100      0.301     0.0301      0.044     0.0212\n",
            "                020101      33816        100     0.0592       0.33     0.0318      0.022\n",
            "                020501      33816        100          0          0    0.00271    0.00184\n",
            "                020301      33816        100    0.00379       0.01    0.00977    0.00621\n",
            "                020102      33816        100          0          0     0.0164     0.0115\n",
            "                020108      33816        100      0.144       0.11     0.0421     0.0278\n",
            "                020111      33816        100          0          0    0.00508    0.00377\n",
            "                020116      33816        100          0          0    0.00192    0.00128\n",
            "                020120      33816        100          0          0    0.00454    0.00289\n",
            "                020123      33816        100     0.0497       0.08     0.0212     0.0124\n",
            "                020130      33816        100          0          0     0.0131    0.00773\n",
            "                020134      33816        100          0          0   0.000747   0.000581\n",
            "                020302      33816        100          0          0     0.0179     0.0114\n",
            "                020316      33816        100     0.0165       0.01    0.00731    0.00379\n",
            "                020323      33816        100      0.101       0.06     0.0449     0.0306\n",
            "                020334      33816        100          0          0    0.00508    0.00321\n",
            "                020502      33816        100        0.1       0.03     0.0785     0.0656\n",
            "                020503      33816        100      0.212       0.17     0.0947     0.0677\n",
            "                020506      33816        100          0          0     0.0609     0.0473\n",
            "                020522      33816        100      0.214       0.15      0.109     0.0857\n",
            "                020523      33816        100     0.0852       0.18     0.0649     0.0508\n",
            "                020723      33816        100      0.293       0.09     0.0895     0.0716\n",
            "                020901      33816        100     0.0335       0.28     0.0502     0.0276\n",
            "                020904      33816        100      0.708     0.0972      0.194      0.105\n",
            "                020908      33816        100     0.0549     0.0011     0.0713     0.0514\n",
            "                020912      33816        100       0.35      0.007      0.114     0.0562\n",
            "                020914      33816        100      0.369       0.04     0.0267      0.018\n",
            "                020915      33816        100      0.296       0.29      0.172      0.128\n",
            "                020919      33816        100          1          0     0.0254     0.0191\n",
            "                020920      33816        100      0.181       0.11     0.0758     0.0287\n",
            "                020923      33816        100          1          0     0.0155     0.0058\n",
            "                030101      33816        100          1          0      0.107     0.0805\n",
            "                030102      33816        100      0.149       0.16      0.116     0.0757\n",
            "                030104      33816        100      0.246       0.07     0.0803     0.0576\n",
            "                030106      33816        100      0.282       0.31      0.182     0.0881\n",
            "                030108      33816        100     0.0575       0.28     0.0683     0.0534\n",
            "                030114      33816        100     0.0674       0.39     0.0874     0.0633\n",
            "                030116      33816        100      0.125       0.23     0.0984     0.0596\n",
            "                030124      33816        100     0.0696       0.46     0.0962     0.0657\n",
            "                030125      33816        100     0.0751       0.44     0.0884     0.0687\n",
            "                030126      33816        100      0.566       0.17       0.33      0.242\n",
            "                030201      33816        100      0.302       0.15       0.12     0.0982\n",
            "                030224      33816        100      0.344       0.17      0.154      0.119\n",
            "                030301      33816        100      0.047       0.01     0.0494     0.0258\n",
            "                030404      33816        100      0.102       0.19      0.065     0.0477\n",
            "                030411      33816        100          1          0     0.0435     0.0337\n",
            "                030418      33816        100     0.0702      0.301     0.0655      0.047\n",
            "                030420      33816        100       0.25       0.04      0.105     0.0377\n",
            "                030421      33816        100      0.304       0.09      0.112     0.0783\n",
            "                030424      33816        100     0.0675       0.16     0.0339     0.0175\n",
            "                030425      33816        100      0.169       0.39      0.139     0.0773\n",
            "                030501      33816        100      0.164       0.32     0.0975     0.0637\n",
            "                030519      33816        100          1          0     0.0296      0.021\n",
            "                030520      33816        100      0.456       0.15      0.149     0.0891\n",
            "                030524      33816        100      0.241        0.2       0.12     0.0686\n",
            "                030525      33816        100      0.113       0.16     0.0695     0.0425\n",
            "                030526      33816        100      0.277       0.02      0.276      0.198\n",
            "                030603      33816        100      0.305       0.16      0.117     0.0776\n",
            "                030625      33816        100          1          0     0.0331     0.0177\n",
            "                030701      33816        100      0.453     0.0334      0.183      0.141\n",
            "                030702      33816        100      0.233       0.01      0.086     0.0625\n",
            "                030703      33816        100     0.0424       0.05     0.0132    0.00961\n",
            "                030705      33816        100      0.789     0.0379      0.204      0.154\n",
            "                030706      33816        100      0.591       0.18      0.176      0.124\n",
            "                030707      33816        100          1          0     0.0299     0.0166\n",
            "                030716      33816        100      0.397       0.29      0.227      0.159\n",
            "                030717      33816        100      0.203       0.26      0.171      0.142\n",
            "                030719      33816        100      0.947       0.07     0.0842     0.0682\n",
            "                030721      33816        100          1          0      0.025     0.0172\n",
            "                030724      33816        100     0.0438        0.1     0.0244     0.0167\n",
            "                030725      33816        100      0.115       0.11     0.0572     0.0404\n",
            "                030729      33816        100      0.355       0.02     0.0896     0.0492\n",
            "                030910      33816        100     0.0937       0.06     0.0221     0.0104\n",
            "                030915      33816        100          0          0     0.0768     0.0455\n",
            "                030916      33816        100          1          0     0.0269     0.0169\n",
            "                030924      33816        100      0.113        0.1     0.0446     0.0238\n",
            "                030925      33816        100          1          0     0.0338     0.0257\n",
            "                031111      33816        100          1          0     0.0679     0.0499\n",
            "                031112      33816        100          1          0      0.152      0.117\n",
            "                031124      33816        100          0          0     0.0292     0.0232\n",
            "                031301      33816        100      0.177       0.33      0.209       0.12\n",
            "                031304      33816        100          0          0     0.0544     0.0336\n",
            "                031324      33816        100     0.0466       0.01     0.0891     0.0629\n",
            "                031503      33816        100          1          0     0.0527     0.0388\n",
            "                040102      33816        100      0.299       0.01     0.0705     0.0533\n",
            "                040303      33816        100          0          0     0.0383     0.0218\n",
            "                040320      33816        100      0.797     0.0394     0.0976      0.073\n",
            "                040501      33816        100      0.109       0.27     0.0961     0.0681\n",
            "                040502      33816        100     0.0357       0.13     0.0215     0.0137\n",
            "                040503      33816        100      0.085       0.46      0.301      0.151\n",
            "                040504      33816        100      0.504       0.43      0.456      0.343\n",
            "                040505      33816        100     0.0623       0.32     0.0917     0.0526\n",
            "                040509      33816        100      0.979      0.952      0.989      0.864\n",
            "                040515      33816        100     0.0853       0.03     0.0262     0.0202\n",
            "                040521      33816        100      0.087       0.33     0.0877     0.0614\n",
            "                050101      33816        100      0.388       0.24      0.183      0.113\n",
            "                050102      33816        100      0.163       0.51      0.164      0.129\n",
            "                050105      33816        100      0.224       0.58      0.248      0.196\n",
            "                050111      33816        100      0.288       0.51      0.234       0.19\n",
            "                050116      33816        100      0.106       0.57      0.106     0.0776\n",
            "                050117      33816        100      0.571        0.2      0.296      0.235\n",
            "                050306      33816        100       0.29        0.2      0.191      0.139\n",
            "                050308      33816        100      0.518       0.19      0.214      0.128\n",
            "                050313      33816        100     0.0355       0.68     0.0413     0.0184\n",
            "                050314      33816        100      0.246       0.14      0.157     0.0851\n",
            "                050315      33816        100     0.0806        0.1     0.0366     0.0207\n",
            "                050316      33816        100          0          0     0.0242     0.0152\n",
            "                050320      33816        100      0.161       0.09      0.082     0.0576\n",
            "                050501      33816        100          0          0     0.0262     0.0149\n",
            "                050519      33816        100      0.101       0.02     0.0278     0.0162\n",
            "                050520      33816        100     0.0289       0.47     0.0299     0.0127\n",
            "                050521      33816        100      0.172       0.19      0.105     0.0491\n",
            "                050522      33816        100      0.365     0.0519     0.0931     0.0457\n",
            "                050701      33816        100          1          0     0.0282      0.012\n",
            "                050702      33816        100     0.0864       0.02     0.0416      0.015\n",
            "                050703      33816        100          1          0     0.0395     0.0145\n",
            "                050713      33816        100      0.418        0.3      0.214      0.104\n",
            "                050721      33816        100          0          0     0.0241     0.0178\n",
            "                050722      33816        100          1          0     0.0448       0.03\n",
            "                050727      33816        100     0.0811       0.02     0.0273     0.0125\n",
            "                050915      33816         28          1          0     0.0126     0.0088\n",
            "                051102      33816        100      0.292      0.029     0.0185     0.0101\n",
            "                051111      33816        100     0.0403    0.00121     0.0157     0.0108\n",
            "                051113      33816        100          0          0     0.0266     0.0141\n",
            "                051308      33816        100      0.219        0.3      0.161      0.118\n",
            "                051311      33816        100      0.142       0.02     0.0714     0.0537\n",
            "                060102      33816        100      0.248       0.16      0.146     0.0921\n",
            "                060104      33816        100     0.0893       0.41     0.0838     0.0417\n",
            "                061909      33816        100      0.389       0.07      0.261      0.154\n",
            "                070101      33816        100      0.255       0.06      0.171      0.098\n",
            "                070109      33816        100     0.0891       0.11     0.0597     0.0386\n",
            "                070112      33816        100      0.279     0.0619      0.148     0.0873\n",
            "                070124      33816        100      0.128        0.6      0.212      0.154\n",
            "                070125      33816        100      0.308       0.28      0.155     0.0972\n",
            "                070301      33816        100          0          0     0.0419     0.0226\n",
            "                070311      33816        100      0.242       0.42       0.21      0.135\n",
            "                070515      33816        100      0.741      0.029      0.108     0.0658\n",
            "                080703      33816        100      0.265       0.04      0.112     0.0564\n",
            "                080705      33816        100     0.0714       0.05     0.0428     0.0252\n",
            "                080711      33816        100          1          0     0.0173     0.0113\n",
            "                080725      33816        100          1          0      0.008    0.00449\n",
            "                090110      33816        100     0.0609       0.02     0.0299     0.0216\n",
            "                090313      33816        100     0.0937       0.03     0.0264     0.0159\n",
            "                090705      33816        100          0          0     0.0497      0.033\n",
            "                090709      33816        100      0.316       0.14      0.113     0.0647\n",
            "                090717      33816        100      0.482       0.01     0.0361      0.023\n",
            "                090719      33816        100      0.114       0.38     0.0972     0.0667\n",
            "                090722      33816        100      0.418      0.151      0.223      0.129\n",
            "                090725      33816        100          1          0     0.0147     0.0102\n",
            "                110102      33816        100      0.377       0.03     0.0644     0.0413\n",
            "                110104      33816        100      0.367       0.01     0.0279     0.0176\n",
            "                110105      33816        100       0.34       0.09      0.104     0.0661\n",
            "                110106      33816        100      0.233       0.14     0.0996     0.0509\n",
            "                110301      33816        100          1          0      0.066     0.0429\n",
            "                110303      33816        100      0.547       0.01     0.0722     0.0454\n",
            "                110304      33816        100      0.122       0.09     0.0455     0.0285\n",
            "                110307      33816        100      0.151       0.09     0.0577     0.0353\n",
            "                110318      33816        100      0.133       0.01     0.0228     0.0107\n",
            "                140115      33816        100      0.463     0.0263      0.371      0.232\n",
            "                150702      33816        100      0.455       0.27      0.245      0.184\n",
            "                160313      33816        100      0.122       0.07     0.0877     0.0432\n",
            "                170202      33816        100       0.57       0.31      0.298       0.22\n",
            "                180105      33816        100      0.744       0.06     0.0838     0.0592\n",
            "                180109      33816        100      0.236      0.201      0.107     0.0725\n",
            "                180123      33816        100       0.45        0.3      0.273      0.185\n",
            "                190103      33816        100          0          0     0.0614     0.0463\n",
            "                190701      33816        100      0.277       0.03     0.0372     0.0236\n",
            "                190902      33816        100     0.0265    0.00557     0.0265     0.0146\n",
            "                200103      33816        100          1          0      0.011    0.00629\n",
            "                200525      33816        100      0.157       0.11     0.0789     0.0511\n",
            "                200702      33816        100      0.195       0.05     0.0222     0.0142\n",
            "                210301      33816        100      0.263       0.17      0.132     0.0788\n",
            "                210304      33816        100      0.813       0.02      0.158     0.0835\n",
            "                210307      33816        100          1          0     0.0038     0.0016\n",
            "                210321      33816        100      0.694       0.05      0.187      0.131\n",
            "                230101      33816        100          1          0     0.0594      0.027\n",
            "                240103      33816        100      0.208       0.31      0.155      0.105\n",
            "                240109      33816        100      0.306       0.29      0.246      0.192\n",
            "                240113      33816        100      0.409       0.06      0.127        0.1\n",
            "                240115      33816        100      0.133       0.47      0.115     0.0802\n",
            "                240307      33816        100      0.411        0.3      0.281      0.194\n",
            "                240309      33816        100     0.0477       0.85      0.176      0.117\n",
            "                240715      33816        100      0.263       0.03      0.118     0.0592\n",
            "                240901      33816        100      0.267       0.45      0.213      0.145\n",
            "                240902      33816        100      0.125       0.14      0.105     0.0757\n",
            "                240905      33816        100      0.312       0.41       0.25      0.162\n",
            "                240907      33816        100      0.156       0.66      0.178      0.111\n",
            "                240909      33816        100      0.325       0.27      0.282      0.139\n",
            "                240913      33816        100      0.319        0.4      0.286      0.201\n",
            "                240916      33816        100      0.322       0.12      0.135     0.0699\n",
            "                240925      33816        100      0.159       0.11       0.16      0.109\n",
            "                241301      33816        100      0.153       0.42      0.126     0.0814\n",
            "                241322      33816        100      0.509      0.124      0.188      0.112\n",
            "                241325      33816        100          0          0     0.0607     0.0422\n",
            "                241501      33816        100          1          0     0.0235     0.0161\n",
            "                241502      33816        100          1          0     0.0349     0.0257\n",
            "                241503      33816        100          1          0     0.0102    0.00741\n",
            "                241507      33816        100          1          0     0.0102     0.0072\n",
            "                241513      33816        100          1          0     0.0528     0.0297\n",
            "                241521      33816        100      0.188       0.01     0.0492     0.0365\n",
            "                241701      33816        100      0.158       0.09     0.0574      0.032\n",
            "                241702      33816        100     0.0355       0.01     0.0181    0.00678\n",
            "                241703      33816        100      0.302       0.49      0.309      0.216\n",
            "                241704      33816        100      0.354        0.1      0.136     0.0665\n",
            "                241705      33816        100      0.147       0.17     0.0938     0.0525\n",
            "                241708      33816        100      0.487       0.79      0.683      0.499\n",
            "                241709      33816        100          1          0     0.0212     0.0114\n",
            "                241712      33816        100          1          0     0.0337     0.0124\n",
            "                241714      33816        100      0.629       0.16      0.346      0.192\n",
            "                241720      33816        100      0.936      0.292      0.538      0.274\n",
            "                241721      33816        100      0.221       0.34      0.168      0.114\n",
            "                241725      33816        100      0.286       0.27      0.182      0.122\n",
            "                250105      33816        100     0.0473       0.06     0.0189     0.0151\n",
            "                250106      33816        100       0.15       0.54      0.142      0.102\n",
            "                250110      33816        100      0.163       0.05     0.0859     0.0728\n",
            "                250118      33816        100          1     0.0278      0.128      0.111\n",
            "                250125      33816        100     0.0941       0.16     0.0536     0.0309\n",
            "                250303      33816        100      0.789       0.15      0.306      0.238\n",
            "                250307      33816        100      0.553       0.35      0.447      0.372\n",
            "                250315      33816        100      0.349      0.301      0.283      0.229\n",
            "                250325      33816        100      0.286       0.06     0.0649     0.0495\n",
            "                250501      33816        100      0.675      0.021       0.27      0.207\n",
            "                250502      33816        100      0.466    0.00932      0.127      0.104\n",
            "                250721      33816        100      0.451     0.0576       0.22      0.193\n",
            "                250726      33816        100          1          0     0.0165     0.0139\n",
            "                260103      33816        100      0.116       0.27      0.116     0.0692\n",
            "                260104      33816        100     0.0417       0.14     0.0243      0.018\n",
            "                260105      33816        100     0.0605       0.23     0.0383     0.0275\n",
            "                260106      33816        100     0.0282        0.5     0.0442     0.0299\n",
            "                260110      33816        100      0.597       0.42      0.433      0.306\n",
            "                260111      33816        100      0.375        0.1      0.139     0.0989\n",
            "                260112      33816        100     0.0473       0.25     0.0329     0.0167\n",
            "                260113      33816        100      0.178       0.32       0.18       0.13\n",
            "                260114      33816        100     0.0425       0.23     0.0353     0.0249\n",
            "                260115      33816        100     0.0341       0.54      0.042     0.0274\n",
            "                260116      33816        100     0.0265       0.17     0.0204     0.0167\n",
            "                260117      33816        100          1          0     0.0655     0.0462\n",
            "                260118      33816        100      0.019       0.64     0.0361     0.0287\n",
            "                260119      33816        100      0.109       0.56      0.126     0.0941\n",
            "                260121      33816        100       0.21       0.13      0.195      0.165\n",
            "                260124      33816        100      0.124       0.25      0.133      0.106\n",
            "                260201      33816        100      0.291     0.0494     0.0299     0.0196\n",
            "                260205      33816        100     0.0382       0.07     0.0424     0.0306\n",
            "                260207      33816        100      0.753     0.0307     0.0931     0.0683\n",
            "                260210      33816        100          0          0     0.0495     0.0269\n",
            "                260212      33816        100          1          0     0.0297     0.0221\n",
            "                260218      33816        100      0.267       0.16      0.104     0.0777\n",
            "                260224      33816        100     0.0741       0.12     0.0293     0.0157\n",
            "                260301      33816        100      0.245       0.31      0.269      0.163\n",
            "                260304      33816        100     0.0668       0.29     0.0753     0.0456\n",
            "                260305      33816        100      0.543       0.38      0.392      0.226\n",
            "                260306      33816        100      0.201       0.08     0.0816     0.0484\n",
            "                260307      33816        100      0.375        0.3      0.231      0.194\n",
            "                260312      33816        100      0.681     0.0644      0.351      0.276\n",
            "                260318      33816        100      0.469       0.49      0.313       0.26\n",
            "                260323      33816        100     0.0423       0.11      0.021     0.0135\n",
            "                260324      33816        100      0.189       0.01      0.174       0.11\n",
            "                260402      33816        100          1          0      0.213      0.163\n",
            "                260403      33816        100      0.213       0.39      0.174      0.128\n",
            "                260404      33816        100     0.0202       0.08     0.0146    0.00907\n",
            "                260405      33816        100      0.209       0.09     0.0712     0.0437\n",
            "                260406      33816        100      0.149       0.15      0.122     0.0926\n",
            "                260407      33816        100      0.106       0.06     0.0522      0.036\n",
            "                260408      33816        100          0          0     0.0751     0.0557\n",
            "                260409      33816        100      0.025       0.54      0.032     0.0167\n",
            "                260410      33816        100      0.075        0.1     0.0501     0.0381\n",
            "                260411      33816        100      0.347       0.02      0.148     0.0895\n",
            "                260412      33816        100     0.0738       0.15     0.0722     0.0431\n",
            "                260413      33816        100      0.227       0.07     0.0916     0.0823\n",
            "                260414      33816        100      0.107       0.13     0.0558     0.0457\n",
            "                260415      33816        100     0.0595       0.41      0.122     0.0921\n",
            "                260416      33816        100     0.0613       0.13     0.0364     0.0272\n",
            "                260417      33816        100          1          0      0.042      0.029\n",
            "                260418      33816        100     0.0187       0.68     0.0319     0.0245\n",
            "                260419      33816        100      0.153       0.09     0.0794     0.0526\n",
            "                260424      33816        100       0.12       0.33      0.119      0.082\n",
            "                260501      33816        100      0.443       0.06     0.0902     0.0637\n",
            "                260504      33816        100      0.224       0.18       0.12     0.0857\n",
            "                260512      33816        100      0.404     0.0204      0.114      0.082\n",
            "                260515      33816        100      0.474     0.0991      0.161      0.137\n",
            "                260516      33816        100      0.219       0.02     0.0737      0.057\n",
            "                260518      33816        100      0.208       0.53      0.208      0.127\n",
            "                260524      33816        100      0.549       0.03      0.216      0.148\n",
            "                260701      33816        100          1          0     0.0498     0.0303\n",
            "                260703      33816        100      0.428     0.0228     0.0573     0.0406\n",
            "                260704      33816        100          1          0      0.022      0.013\n",
            "                260705      33816        100     0.0855       0.04     0.0273     0.0162\n",
            "                260715      33816        100      0.112       0.03     0.0716     0.0385\n",
            "                260719      33816        100          1          0     0.0571     0.0422\n",
            "                260720      33816        100          1          0     0.0236     0.0178\n",
            "                260725      33816        100       0.45       0.02      0.064     0.0436\n",
            "                261101      33816        100      0.072       0.05     0.0622     0.0313\n",
            "                261102      33816        100     0.0973       0.09      0.042     0.0284\n",
            "                261103      33816        100     0.0587       0.32     0.0466     0.0333\n",
            "                261107      33816        100       0.06       0.04      0.027     0.0129\n",
            "                261108      33816        100      0.105       0.21     0.0475     0.0263\n",
            "                261109      33816        100      0.136      0.191     0.0647     0.0361\n",
            "                261110      33816        100      0.194      0.108     0.0882      0.045\n",
            "                261111      33816        100      0.109       0.17       0.06     0.0484\n",
            "                261112      33816        100     0.0183       0.43       0.06     0.0477\n",
            "                261113      33816        100     0.0522       0.29      0.067     0.0301\n",
            "                261114      33816        100          1          0     0.0947       0.05\n",
            "                261121      33816        100     0.0203   0.000405      0.168       0.12\n",
            "                261122      33816        100        0.7       0.53      0.539      0.202\n",
            "                261125      33816        100     0.0197       0.07     0.0152    0.00958\n",
            "                261301      33816        100     0.0517      0.206     0.0239     0.0171\n",
            "                261325      33816        100          0          0    0.00743    0.00463\n",
            "                261501      33816        100     0.0822       0.06     0.0527     0.0388\n",
            "                261509      33816        100      0.211       0.44      0.208      0.146\n",
            "                261511      33816        100          1          0     0.0422     0.0276\n",
            "                261520      33816        100      0.463    0.00927     0.0998     0.0618\n",
            "                261525      33816        100          0          0      0.116      0.106\n",
            "                270106      33816        100      0.373     0.0477     0.0729     0.0562\n",
            "                270112      33816        100     0.0869       0.06     0.0496      0.032\n",
            "                270125      33816        100      0.628     0.0342      0.124     0.0993\n",
            "                270302      33816        100     0.0673       0.09     0.0151       0.01\n",
            "                270303      33816        100     0.0869       0.03     0.0213     0.0114\n",
            "                270311      33816        100     0.0499       0.05     0.0304     0.0148\n",
            "                270312      33816        100          0          0     0.0477     0.0309\n",
            "                270315      33816        100     0.0454       0.01     0.0114    0.00693\n",
            "                270501      33816        100          1          0     0.0189     0.0131\n",
            "                270502      33816        100      0.185       0.29      0.108      0.067\n",
            "                270503      33816        100     0.0787       0.34     0.0625     0.0401\n",
            "                270504      33816        100          1          0     0.0442     0.0308\n",
            "                270505      33816        100          1          0     0.0749     0.0555\n",
            "                270507      33816        100      0.161        0.4      0.115     0.0726\n",
            "                270508      33816        100     0.0288       0.12      0.019     0.0138\n",
            "                270510      33816        100     0.0611       0.02     0.0201     0.0137\n",
            "                270511      33816        100      0.102       0.41      0.132     0.0786\n",
            "                270512      33816        100     0.0433       0.02     0.0189     0.0134\n",
            "                270514      33816        100       0.35       0.13      0.109     0.0703\n",
            "                270519      33816        100     0.0867       0.45      0.134     0.0913\n",
            "                270521      33816        100      0.237       0.01      0.118     0.0924\n",
            "                270522      33816        100      0.112        0.1     0.0451     0.0351\n",
            "                270524      33816          2          0          0          0          0\n",
            "                270525      33816        100          1          0     0.0421     0.0332\n",
            "                270526      33816        100          1          0      0.108     0.0723\n",
            "                270711      33816        100          1          0     0.0108     0.0071\n",
            "                270901      33816        100     0.0965       0.44      0.103     0.0498\n",
            "                270902      33816        100      0.107       0.42      0.116     0.0708\n",
            "                270903      33816        100     0.0354       0.05     0.0302     0.0173\n",
            "                270904      33816        100     0.0834       0.11     0.0407     0.0201\n",
            "                270905      33816        100      0.093       0.04     0.0271     0.0169\n",
            "                270906      33816        100      0.136       0.02     0.0428     0.0245\n",
            "                270907      33816        100     0.0511       0.03     0.0246     0.0146\n",
            "                270908      33816        100       0.16       0.23      0.112     0.0775\n",
            "                270909      33816        100     0.0259       0.03    0.00728    0.00348\n",
            "                270910      33816        100      0.038       0.01      0.036     0.0204\n",
            "                270911      33816        100      0.161       0.25      0.108     0.0772\n",
            "                270912      33816        100      0.154       0.02      0.134     0.0896\n",
            "                270913      33816        100      0.114       0.47       0.11     0.0782\n",
            "                270914      33816        100      0.061       0.07      0.055     0.0396\n",
            "                270915      33816        100     0.0708       0.39     0.0688     0.0474\n",
            "                270916      33816        100     0.0947       0.16     0.0793     0.0332\n",
            "                270917      33816        100      0.532       0.28      0.353      0.184\n",
            "                270918      33816        100      0.161       0.15      0.111     0.0642\n",
            "                270919      33816        100      0.082        0.3     0.0814     0.0573\n",
            "                270920      33816        100      0.298       0.43       0.32      0.232\n",
            "                270921      33816        100       0.29       0.24      0.194      0.125\n",
            "                270922      33816        100       0.22      0.147      0.141     0.0856\n",
            "                270923      33816        100      0.288        0.4      0.212      0.151\n",
            "                270924      33816        100      0.192        0.2       0.13     0.0877\n",
            "                270925      33816        100      0.202     0.0331     0.0715     0.0392\n",
            "                270926      33816        100      0.636     0.0527      0.135      0.082\n",
            "                280303      33816        100          1          0     0.0102    0.00813\n",
            "                290101      33816        100          1          0     0.0525     0.0323\n",
            "Speed: 0.0ms preprocess, 1.2ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mv10/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# model = YOLO(f\"{MODEL}/train/weights/last.pt\")\n",
        "model = YOLO(\"yolov8m.yaml\")\n",
        "\n",
        "results = model.train(\n",
        "    data=\"/content/data/yolocustom.yaml\",\n",
        "    imgsz=IMGSZ,\n",
        "    epochs=EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    patience=5,\n",
        "    workers=16,\n",
        "    device=0,\n",
        "    exist_ok=True,\n",
        "    project=f\"{MODEL}\",\n",
        "    name=\"train\",\n",
        "    seed=SEED,\n",
        "    pretrained=False,\n",
        "    resume= True,\n",
        "    optimizer=\"Adam\",\n",
        "    lr0=1e-3,\n",
        "    augment=True,\n",
        "    val=True,\n",
        "    cache=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
